"""
Implement a processing pipeline for Spark DataFrames as a Pandas UDF
"""

from os import getpid
import random

import numpy as np
import pandas as pd

from pyspark.sql import types

from typing import Dict, List, Iterator

from .utils import logging as logging
from .utils.misc import import_object
from .utils.exception import SetupException


BASE_PATH = "dps.spark_df.udf."


class UdfProcessor:
    """
    The main UDF object. Acts as a Callable, so it can be added as a
    Pandas UDF by using it in mapInPandas
    """

    def __init__(self, config: List[Dict], seed: int = None,
                 logconfig: Dict = None):
        """
          :param config: the list of processor configurations
          :param seed: a seed to initialize random generation
          :param logconfig: configuration for the logger
        Store the configuration.
        Delay instantiating objects until call time, so that they are
        created in executors
        """
        # Check config
        for n, preproc in enumerate(config, start=1):
            if "class" not in preproc:
                raise SetupException(f"preprocessor config without class definition ({n})")
        # Store config
        self.config = config
        self.logconfig = logconfig
        self.seed = seed
        # Look up preprocessors
        self.proc = []
        for preproc in self.config:
            classname = preproc.pop("class")
            cls = import_object(BASE_PATH + classname)
            self.proc.append((cls, preproc))
        # Delay actual initialization until first use
        self.init = False


    def schema(self, schema_in: types.StructType) -> types.StructType:
        """
        Return the DataFrame schema that will be generated by this UDF, by addding
        all new columns to be created
          :param schema_in: schema of the input dataframe
          :return: schema of the output dataframe
        """
        # Do not _append_ to the fields in schema_in, create a new list
        newfields = []
        for preproc in self.config:
            columns = preproc.get("columns")
            if not columns:
                continue
            if isinstance(columns, str):
                columns = [columns]
            for c in columns:
                try:
                    t = getattr(types, c["type"])
                    newfields.append(types.StructField(c["name"], t(), True))
                except KeyError as e:
                    raise SetupException("Missing field in column config: {}",
                                         e) from e
                except AttributeError as e:
                    raise SetupException("Spark type not found: {}",
                                         c["type"]) from e

        allfields = schema_in.fields + newfields
        return types.StructType(allfields)


    def _init_obj(self):
        """
        Initialize the object
        """
        pid = getpid()

        # Logging
        if self.logconfig:
            logging.basicConfig(**self.logconfig)
            logging.getLogger("py4j").setLevel(logging.ERROR)
            logging.getLogger("py4j.java_gateway").setLevel(logging.ERROR)

        self.logger = logging.getLogger(__name__)
        self.logger.info("preproc %d init", pid)

        # Set the random seed, if given
        if self.seed:
            random.seed(self.seed)
            np.random.seed(self.seed)

        # Instantiate all preprocessors
        self.proc = [cls(conf) for cls, conf in self.proc]
        self.logger.info("preproc %d modules = %s", pid,
                         ', '.join(map(str, self.proc)))

        # Mark as initialized
        self.init = True


    def __call__(self, dfl: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:
        """
        The UDF entry point. Do the processing.
        If necessary, initialize objects
        """
        # Initialize upon first call
        if not self.init:
            self._init_obj()

        self.logger.debug("executor preproc %d", getpid())

        # Process all DataFrames
        for df in dfl:
            try:
                self.logger.debug("UDF rows=%d", len(df))

                # Apply preprocessors
                for proc in self.proc:
                    df = proc(df)

                yield df

            except Exception as e:
                self.logger.error("Error in UDF: %s", e)

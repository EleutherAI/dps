# Configuration file for the DataFrame-based preprocessing of documents
# ---------------------------------------------------------------------

general:
  # Set the random seed
  seed: 1

# -----------------------------------------------------------------------

# Input/output files
io:

  # Define data source (JSONL files)
  source:
    base: 's3a://polyglot-romance-west/'
    format: json
    paths:
      - 'es/2_clean/gutenberg/DATA/'
    options:
      pathGlobFilter: '*.jsonl.bz2'
      encoding: 'utf-8'
    
  # Define data destination
  dest:
    format: jsonl
    name: 's3a://polyglot-romance-west/TEST/test_preproc_output'
    mode: overwrite
    partitions: 5
    options:
      compression: bzip2

# -----------------------------------------------------------------------

# Parameters to configure the spark session
spark:
  master: 'local[20]'
  partitions: 20
  driver:
    memory: 4g
  executor:
    cores: 20
    memory: 20g
  config:
    spark.sql.files.ignoreCorruptFiles: "true"
    spark.sql.execution.arrow.maxRecordsPerBatch: 2000
  s3_config:
    endpoint: s3.us-west-2.amazonaws.com
  logging:
    level: WARN


# Configuration for logging
logging:
  reset: true
  logconfig:
    filename: '/tmp/preproc.log'
    level: DEBUG


# -----------------------------------------------------------------------

# Define preprocessing steps
preprocess:

  langfilter:
    chunk_size: 20
    max_chunks: 10
    min_lang_score: 0.8
    min_chunk_ratio: 0.6
    min_lang_ratio: 0.3
    keep_lang:
      - es
      - fr
      - pt
      - it
      - ro
